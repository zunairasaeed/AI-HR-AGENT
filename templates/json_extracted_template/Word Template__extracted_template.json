{
  "template_name": "Word Template",
  "sections": {
    "Personal Info": {},
    "Profile": {
      "summary": "With over 7+ years of experience in the Data Domain, specializing in Microsoft Cloud technologies. I have led projects in various industries, including device calibration, food, pharmaceuticals, healthcare, banking, and retail. My expertise includes designing and developing data pipelines, implementing Medallion architecture, transitioning to Microsoft Fabric, real-time data monitoring, and integrating machine learning models. I am proficient in a wide range of tools and technologies, such as Microsoft Fabric, SQL Server, Oracle DB, Azure Git, Blob Storage, SparkSQL, PySpark, Azure Data Factory, Databricks, Power BI, Talend, Vertica, Apache Airflow, and Google BigQuery."
    },
    "Skills": {
      "Technical": [
        "Microsoft Fabric",
        "SQL Server",
        "Oracle DB",
        "Azure Git",
        "Blob Storage",
        "SparkSQL",
        "PySpark",
        "Azure Data Factory",
        "Databricks",
        "Power BI",
        "Talend",
        "Vertica",
        "Apache Airflow",
        "Google BigQuery"
      ],
      "Platforms": [],
      "Tools": []
    },
    "Certifications": [
      "Vertica Certified Professional Essentials 10 Certificate"
    ],
    "Functional Skills": [
      "Agile",
      "Scrum",
      "Functional Analyst"
    ],
    "Business Sector": [
      "IT consultancy"
    ],
    "Languages": [],
    "Work Experience": [
      {
        "company": "Trescal",
        "title": "Senior Data Engineer",
        "dates": "11/2024 till Present (5 months)",
        "responsibilities": [
          "Designed and developed robust pipelines to handle diverse data sources, including JSON files, SQL Server, and Oracle databases from various countries.",
          "Led the implementation of scalable data pipelines, ensuring accurate and efficient data processing.",
          "Managed and standardized datasets from multiple countries, ensuring seamless integration and consistency.",
          "Applied advanced data transformation techniques to streamline data pipelines and enhance performance.",
          "Worked closely with team members and stakeholders to ensure technical solutions aligned with the client's business needs."
        ],
        "tech_stack": [
          "Microsoft Fabric",
          "SQL Server",
          "Oracle DB",
          "JSON"
        ]
      },
      {
        "company": "AirLife",
        "title": "Senior Data Engineer",
        "dates": "10/2023 till 10/2024 (1 year)",
        "responsibilities": [
          "Led the transition of a multi-billion-dollar client from Azure tech stack (ADF, Databricks, Azure SQL, Power BI) to a unified Microsoft Fabric platform.",
          "Designed and implemented pipelines, data activator/reflex for triggers, and notebooks for comprehensive data transformation and management.",
          "Seamlessly integrated on-premises databases into the Fabric environment, ensuring consistent data flow.",
          "Developed and optimized curated data layers, enabling robust, real-time analytics and reporting through Power BI.",
          "Worked closely with client stakeholders to ensure a smooth migration and provided ongoing support for system optimization."
        ],
        "tech_stack": [
          "Microsoft Fabric",
          "Azure Data Factory",
          "Databricks",
          "Azure SQL",
          "Power BI",
          "On-Premises Integration",
          "Azure Storage Accounts",
          "Blob",
          "ADLS Gen2"
        ]
      },
      {
        "company": "Subway IPC",
        "title": "Senior Data Engineer",
        "dates": "08/2022 till 09/2023 (1.1 years)",
        "responsibilities": [
          "Designed and implemented a Medallion architecture using 12 lakehouses, divided into 3 layers (Bronze, Silver, Gold) and 3 ETL control planes.",
          "Transitioned from EnterWorks and AWS to Microsoft Fabric, leveraging Blob storage and lakehouse shortcuts for data ingestion.",
          "Decrypted sensitive data, implemented SCD Type 3, and set up Microsoft Teams notifications for delta table updates.",
          "Utilized deployment pipelines and Azure Git for workspaces and version control. Implemented Fabric Accelerator for job audits and logging.",
          "Enabled streamlined data processes, enhancing the client\u2019s ability to manage and utilize data across environments (DEV, UAT, PRD)."
        ],
        "tech_stack": [
          "Microsoft Fabric",
          "Azure Git",
          "Blob Storage",
          "SCD Type 3",
          "Microsoft Teams",
          "EnterWorks",
          "AWS",
          "SparkSQL",
          "PySpark",
          "Spark",
          "Azure KeyVault"
        ]
      },
      {
        "company": "AKDN",
        "title": "Data Engineer",
        "dates": "09/2021 till 07/2022 (11 months)",
        "responsibilities": [
          "Developed Power BI dashboards for real-time vaccine data monitoring, integrating health records and vaccination databases.",
          "Implemented ETL pipelines from Vertica, ensuring accurate and timely reporting. Optimized the data model for real-time insights.",
          "Focused on KPIs such as Vaccination Coverage, Adverse Events, Vaccine Uptake Trends, and Antenatal Care Compliance.",
          "Utilized DAX for complex calculations and Power Query for data extraction and transformation.",
          "Adhered to strict data governance policies, ensuring compliance with healthcare regulations."
        ],
        "tech_stack": [
          "Power BI",
          "Talend",
          "Vertica",
          "DAX",
          "Power Query"
        ]
      },
      {
        "company": "Faysal Bank",
        "title": "Data Engineer",
        "dates": "08/2019 till 09/2021 (2.1 years)",
        "responsibilities": [
          "Collaborated on a high-impact cash optimization project, improving cash management and forecasting accuracy.",
          "Designed and optimized SQL queries and ETL pipelines from banking systems, enabling advanced dataset creation for machine learning models.",
          "Integrated ML models for cash forecasting, significantly reducing operational costs.",
          "Played a key role in project implementation, providing expertise in data engineering and ongoing system maintenance."
        ],
        "tech_stack": [
          "SQL",
          "ETL",
          "Machine Learning Models",
          "Data Warehousing"
        ]
      },
      {
        "company": "24/7",
        "title": "Data Engineer",
        "dates": "01/2017 till 07/2019 (2.7 years)",
        "responsibilities": [
          "Orchestrated ETL workflows with Apache Airflow, designed Airflow DAGs for efficient data extraction, and implemented scalable data warehousing solutions with Google BigQuery.",
          "Developed and optimized data retrieval models and supported ML models for inventory optimization and demand forecasting.",
          "Contributed to the development of a credit-based inventory supply model, resulting in significant cost savings.",
          "Ensured data security and compliance with industry regulations, providing ongoing support for ETL processes and ML models."
        ],
        "tech_stack": [
          "Apache Airflow",
          "Google BigQuery",
          "ETL",
          "Machine Learning"
        ]
      }
    ],
    "Education": [],
    "Projects": []
  }
}